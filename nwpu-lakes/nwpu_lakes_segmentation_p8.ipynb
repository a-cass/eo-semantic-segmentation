{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation of Water using U-Net\n",
    "# Part 8 - Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from unetlib.metrics import BinaryMeanIoU\n",
    "from unetlib.model import UNet_BN\n",
    "from unetlib.preprocessing import make_dataframes_for_flow, make_img_msk_flows\n",
    "import unetlib.visualisation as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagery directories\n",
    "nwpu_data_dir = 'nwpu_lake_images/data/'\n",
    "nwpu_mask_dir = 'nwpu_lake_images/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'model_outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've selected my best model I can test it to see how it performs on the unseen test data to get a final evaluation. I no longer have need for the validation set so I can now retrain the model using the full training set i.e. no validation split.\n",
    "\n",
    "This *should* help with model generalisation as it will be trained on more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model with no validation\n",
    "opt = 'RMSProp'\n",
    "lr = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "runs = 1\n",
    "losses, ious = [], []\n",
    "\n",
    "for i in range(runs):\n",
    "    model = UNet_BN(n_filters=64, n_blocks=4, bn_pos='before', model_name='final')\n",
    "    compile_model(model, opt, lr)\n",
    "    \n",
    "    # Generate filename for output files\n",
    "    opt_conf = model.optimizer.get_config()\n",
    "    o_name = opt_conf['name']\n",
    "    o_lr = f\"{tf.keras.backend.eval(opt_conf['learning_rate']):.3f}\"\n",
    "    base_fn = os.path.join(out_dir, f\"{model.name}_{o_name}_lr{o_lr}_bs{batch_size}{{}}\")\n",
    "    \n",
    "    \n",
    "    ## Configure callbacks\n",
    "    # Checkpointer\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(base_fn.format('.weights.h5'),\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_weights_only=True\n",
    "                                                      )\n",
    "    # Timer\n",
    "    timer = TrainingTimer()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(patience=50)\n",
    "    \n",
    "    \n",
    "    callbacks=[early_stop, checkpointer, timer]\n",
    "    \n",
    "    # Split the test/train data\n",
    "    (train_img_df, train_msk_df,\n",
    "     test_img_df, test_msk_df) = make_dataframes_for_flow(nwpu_data_dir,\n",
    "                                                          nwpu_mask_dir,\n",
    "                                                          test_size=0.25,\n",
    "                                                          random_state=42\n",
    "                                                          )\n",
    "\n",
    "    # Split the training data into train and validation generators\n",
    "    # with augmentation applied to the training data only\n",
    "    aug_dict = {'rotation_range': 90,\n",
    "                'horizontal_flip': True,\n",
    "                'vertical_flip': True,\n",
    "                'width_shift_range': 0.15,\n",
    "                'height_shift_range': 0.15,\n",
    "                'zoom_range': 0.25\n",
    "                }\n",
    "\n",
    "    (train_gen, train_fps) = make_img_msk_flows(train_img_df, train_msk_df,\n",
    "                                                nwpu_data_dir, nwpu_mask_dir,\n",
    "                                                val_split=0, rescale=1 / 255.,\n",
    "                                                aug_dict=aug_dict,\n",
    "                                                batch_size=batch_size\n",
    "                                               )\n",
    "\n",
    "    # Compute steps per epoch\n",
    "    train_steps = int(np.ceil(len(train_fps) / batch_size))\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_gen, epochs=epochs, steps_per_epoch=train_steps,\n",
    "                        callbacks=callbacks\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
