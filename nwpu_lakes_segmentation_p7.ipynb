{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation of Water using U-Net\n",
    "# Part 7 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, save_img\n",
    "import numpy as np\n",
    "import json, os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "\n",
    "from unetlib.metrics import BinaryMeanIoU\n",
    "from unetlib.model import UNet_BN\n",
    "from unetlib.preprocessing import get_lakes_with_masks, make_dataframes_for_flow, make_img_msk_flows\n",
    "import unetlib.visualisation as vs\n",
    "from unetlib.pipelines import train_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the learning rate hyperparameter, and any others for that matter, a selection of values should be tried and whichever yields the best validation loss should be kept. It is not practical or efficient to test every possible value so a common strategy in the literature is to test powers of 10 e.g. 0.001, 0.01, 0.1 etc.\n",
    "\n",
    "Another common approach is to allow the learning rate to be decreased during the training process. This means that earlier steps can make larger movements but as the model converges, the learning rate gets smaller so small steps can me made to avoid overshooting the minimum. One way of implementing this is to use a learning rate scheduler to reduce the rate after a certain number of epochs, though it can be difficult to determine at which epochs the rate should be reudced. An alternative is to reduce the learning rate whenever the loss doesnt improve for a certain amount of time.\n",
    "\n",
    "The `RMSProp` optimiser i'm using also has a `momentum` parameter. This essentially allows gradient descent to build up speed and can help pass local minima or saddle points. RMSProp also includes a dampening factor, `rho` which helps slow the process to avoid overshooting the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up convergence - batch norm / learning rate / momentum\n",
    "\n",
    "Dropout (if overfitting)\n",
    "\n",
    "Activations - could try sigmoid or tanh (adjut batch norm appropriately)\n",
    "\n",
    "Ensemble? e.g. train several smaller models and average the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagery directories\n",
    "nwpu_data_dir = 'nwpu_lake_images/data/'\n",
    "nwpu_mask_dir = 'nwpu_lake_images/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, nwpu_data_dir, nwpu_mask_dir, callbacks=None,\n",
    "               batch_size=16, epochs=100):\n",
    "    \"\"\"\n",
    "    Conveniennce wrapper to train a model on the NWPU data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "    nwpu_data_dir\n",
    "    nwpu_mask_dir\n",
    "    callbacks\n",
    "    batch_size\n",
    "    epochs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the test/train data\n",
    "    (train_img_df, train_msk_df,\n",
    "     test_img_df, test_msk_df) = make_dataframes_for_flow(nwpu_data_dir,\n",
    "                                                          nwpu_mask_dir,\n",
    "                                                          test_size=0.25,\n",
    "                                                          random_state=42\n",
    "                                                          )\n",
    "\n",
    "    # Split the training data into train and validation generators\n",
    "    # with augmentation applied to the training data only\n",
    "    aug_dict = {'rotation_range': 90,\n",
    "                'horizontal_flip': True,\n",
    "                'vertical_flip': True,\n",
    "                'width_shift_range': 0.15,\n",
    "                'height_shift_range': 0.15,\n",
    "                'zoom_range': 0.25\n",
    "                }\n",
    "\n",
    "    (train_gen, val_gen,\n",
    "     train_fps, val_fps) = make_img_msk_flows(train_img_df, train_msk_df,\n",
    "                                              nwpu_data_dir, nwpu_mask_dir,\n",
    "                                              val_split=0.3, rescale=1 / 255.,\n",
    "                                              aug_dict=aug_dict,\n",
    "                                              batch_size=batch_size\n",
    "                                              )\n",
    "\n",
    "    # Compute steps per epoch\n",
    "    train_steps = int(np.ceil(len(train_fps) / batch_size))\n",
    "    val_steps = int(np.ceil(len(val_fps) / batch_size))\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_gen, epochs=epochs, steps_per_epoch=train_steps,\n",
    "                        validation_data=val_gen, validation_steps=val_steps,\n",
    "                        callbacks=callbacks\n",
    "                        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingTimer(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Times the models training process.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.logs = {}\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.logs['start_time'] = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.logs['stop_time'] = time.time()\n",
    "        self.logs['runtime'] = self.logs['stop_time'] - self.logs['start_time']\n",
    "        \n",
    "    def runtime_seconds(self):\n",
    "        return self.logs.get('runtime', 0.)\n",
    "    \n",
    "    def runtime_minutes(self):\n",
    "        return self.runtime_seconds() / 60\n",
    "    \n",
    "    def runtime_hours(self):\n",
    "        return self.runtime_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimiser, lr=None):\n",
    "    \"\"\"Convenience function to compile model\n",
    "    \n",
    "    \"\"\"\n",
    "    # configure optimiser\n",
    "    if isinstance(optimiser, str):\n",
    "        optimiser = tf.keras.optimizers.get(optimiser)\n",
    "        if lr is not None:\n",
    "            optimiser.lr = lr\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[BinaryMeanIoU(threshold=0.5)]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepwide_f64_b4_bnbefore_RMSprop_lr0.001{}\n",
      "deepwide_f64_b4_bnbefore_RMSprop_lr0.010{}\n",
      "deepwide_f64_b4_bnbefore_RMSprop_lr0.100{}\n",
      "deepwide_f64_b4_bnbefore_Adam_lr0.001{}\n",
      "deepwide_f64_b4_bnbefore_Adam_lr0.010{}\n",
      "deepwide_f64_b4_bnbefore_Adam_lr0.100{}\n"
     ]
    }
   ],
   "source": [
    "train_times = {}\n",
    "optimisers = ['RMSProp', 'Adam']\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "save_dir = 'model_outputs/'\n",
    "\n",
    "for opt, lr in product(optimisers, learning_rates):\n",
    "    model = UNet_BN(n_filters=64, n_blocks=4, bn_pos='before', model_name='deepwide')\n",
    "    compile_model(model, opt, lr)\n",
    "    \n",
    "    # Generate filename for output files\n",
    "    opt_conf = model.optimizer.get_config()\n",
    "    o_name = opt_conf['name']\n",
    "    o_lr = f\"{tf.keras.backend.eval(opt_conf['learning_rate']):.3f}\"\n",
    "    base_fn = os.path.join(save_dir, f\"{model.name}_{o_name}_lr{o_lr}{{}}\")\n",
    "    \n",
    "    ## Configure callbacks\n",
    "    # Checkpointer\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(base_fn.format('.weights.h5'),\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_weights_only=True\n",
    "                                                      )\n",
    "    # Timer\n",
    "    timer = TrainingTimer()\n",
    "    \n",
    "    callbacks = [checkpointer, timer]\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    history = train_unet(model, nwpu_data_dir, nwpu_mask_dir,\n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    # Update training times dictionary\n",
    "    train_times[base_fn.format('')] = timer\n",
    "    \n",
    "    # Save history to pickle\n",
    "    with open(base_fn.format('.history.pickle'), 'wb') as f:\n",
    "        pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
